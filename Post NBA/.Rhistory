library(readxl)
library(tidyr)
library(dplyr)
library(ggplot2)
library(lubridate)
library(plyr)
# @İbrahim Onur Şerbetçi 2019
#First cleaning, second q1,third sidcar tries, fourth textmining, fifth q3, sixth some tries, and last ml
nba <- read_excel("Desktop/Data Blog/Blogpost 3 NBA/NBA.xlsx")
View(nba)
#Let's review data generally
summary(nba)
head(nba)
tail(nba)
names(nba)
summary(nba$LikeCount)
#####################################
#First to do that change unpractical columns to categorical variable
nba$PostType.f = as.factor(nba$PostType)
nba$PostItemType.f = as.factor(nba$PostItemType)
summary(nba$PostItemType.f)
summary(nba$PostType.f)
#First column is unnecessary
nba = nba[,-1]
#Height and Width is unnecessary as seperated we unite these two columns
nba = unite(nba,"Height-Width",c("Height","Width"),sep = "x")
names(nba)[5] = "Resolution"
#Differ by post code
nba_by_postcode= nba[,-5]
nba_by_postcode = unique(nba_by_postcode)
nba_by_postcode = nba_by_postcode[,-12]
#Like/View column
nba_by_postcode$l_v_percent=if_else(nba_by_postcode$LikeCount/nba_by_postcode$ViewCount != Inf,
nba_by_postcode$LikeCount/nba_by_postcode$ViewCount,
NULL)
#like count by time
nba_by_postcode$ViewCount_null=if_else(nba_by_postcode$ViewCount!=0,
nba_by_postcode$ViewCount,
NULL)
#First Question
#first question
library("tm")
library("SnowballC")
library("wordcloud")
library("RColorBrewer")
textMining=function(list) {
text_caption = list
docs_caption <- Corpus(VectorSource(text_caption))
inspect(docs_caption)
#Text Transformation
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs_caption <- tm_map(docs_caption, toSpace, "/")
docs_caption <- tm_map(docs_caption, toSpace, "@")
docs_caption <- tm_map(docs_caption, toSpace, "\\|")
docs_caption <- tm_map(docs_caption, toSpace, ",")
#Cleaning the tex
# Convert the text to lower case
docs_caption <- tm_map(docs_caption, content_transformer(tolower))
# Remove numbers
docs_caption <- tm_map(docs_caption, removeNumbers)
# Remove english common stopwords
docs_caption <- tm_map(docs_caption, removeWords, stopwords("english"))
# Remove your own stop word
# specify your stopwords as a character vector
docs_caption <- tm_map(docs_caption, removeWords, c("blabla1", "blabla2"))
# Remove punctuations
docs_caption <- tm_map(docs_caption, removePunctuation)
# Eliminate extra white spaces
docs_caption <- tm_map(docs_caption, stripWhitespace)
# Text stemming
#docs_caption <- tm_map(docs_caption, stemDocument)
#Build a term-document matrix
dtm <- TermDocumentMatrix(docs_caption)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
d$dtm = dtm
return(d)
}
mention_freq = textMining(nba_by_postcode$Mentions)
library(readxl)
library(tidyr)
library(dplyr)
library(ggplot2)
library(lubridate)
library(plyr)
# @İbrahim Onur Şerbetçi 2019
#First cleaning, second q1,third sidcar tries, fourth textmining, fifth q3, sixth some tries, and last ml
nba <- read_excel("Desktop/Data Blog/Blogpost 3 NBA/NBA.xlsx")
View(nba)
#Let's review data generally
summary(nba)
head(nba)
tail(nba)
names(nba)
summary(nba$LikeCount)
#####################################
#First to do that change unpractical columns to categorical variable
nba$PostType.f = as.factor(nba$PostType)
nba$PostItemType.f = as.factor(nba$PostItemType)
summary(nba$PostItemType.f)
summary(nba$PostType.f)
#First column is unnecessary
nba = nba[,-1]
#Height and Width is unnecessary as seperated we unite these two columns
nba = unite(nba,"Height-Width",c("Height","Width"),sep = "x")
names(nba)[5] = "Resolution"
#Differ by post code
nba_by_postcode= nba[,-5]
nba_by_postcode = unique(nba_by_postcode)
nba_by_postcode = nba_by_postcode[,-12]
#Like/View column
nba_by_postcode$l_v_percent=if_else(nba_by_postcode$LikeCount/nba_by_postcode$ViewCount != Inf,
nba_by_postcode$LikeCount/nba_by_postcode$ViewCount,
NULL)
#like count by time
nba_by_postcode$ViewCount_null=if_else(nba_by_postcode$ViewCount!=0,
nba_by_postcode$ViewCount,
NULL)
#First Question
#first question
nba <- read_excel("Desktop/Data Blog/Blogpost 3 NBA/NBA.xlsx")
library(readxl)
library(tidyr)
library(dplyr)
library(ggplot2)
library(lubridate)
library(plyr)
# @İbrahim Onur Şerbetçi 2019
#First cleaning, second q1,third sidcar tries, fourth textmining, fifth q3, sixth some tries, and last ml
nba <- read_excel("~/Desktop/Data Blog/Blogpost 3 NBA/NBA.xlsx")
View(nba)
#Let's review data generally
summary(nba)
head(nba)
tail(nba)
names(nba)
summary(nba$LikeCount)
#####################################
#First to do that change unpractical columns to categorical variable
nba$PostType.f = as.factor(nba$PostType)
nba$PostItemType.f = as.factor(nba$PostItemType)
summary(nba$PostItemType.f)
summary(nba$PostType.f)
#First column is unnecessary
nba = nba[,-1]
#Height and Width is unnecessary as seperated we unite these two columns
nba = unite(nba,"Height-Width",c("Height","Width"),sep = "x")
names(nba)[5] = "Resolution"
#Differ by post code
nba_by_postcode= nba[,-5]
nba_by_postcode = unique(nba_by_postcode)
nba_by_postcode = nba_by_postcode[,-12]
#Like/View column
nba_by_postcode$l_v_percent=if_else(nba_by_postcode$LikeCount/nba_by_postcode$ViewCount != Inf,
nba_by_postcode$LikeCount/nba_by_postcode$ViewCount,
NULL)
#like count by time
nba_by_postcode$ViewCount_null=if_else(nba_by_postcode$ViewCount!=0,
nba_by_postcode$ViewCount,
NULL)
#First Question
#first question
#textmining second question textmining
########################
#Here, I'm going to try introduce into text mining :)
library("tm")
library("SnowballC")
library("wordcloud")
library("RColorBrewer")
textMining=function(list) {
text_caption = list
docs_caption <- Corpus(VectorSource(text_caption))
inspect(docs_caption)
#Text Transformation
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs_caption <- tm_map(docs_caption, toSpace, "/")
docs_caption <- tm_map(docs_caption, toSpace, "@")
docs_caption <- tm_map(docs_caption, toSpace, "\\|")
docs_caption <- tm_map(docs_caption, toSpace, ",")
#Cleaning the tex
# Convert the text to lower case
docs_caption <- tm_map(docs_caption, content_transformer(tolower))
# Remove numbers
docs_caption <- tm_map(docs_caption, removeNumbers)
# Remove english common stopwords
docs_caption <- tm_map(docs_caption, removeWords, stopwords("english"))
# Remove your own stop word
# specify your stopwords as a character vector
docs_caption <- tm_map(docs_caption, removeWords, c("blabla1", "blabla2"))
# Remove punctuations
docs_caption <- tm_map(docs_caption, removePunctuation)
# Eliminate extra white spaces
docs_caption <- tm_map(docs_caption, stripWhitespace)
# Text stemming
#docs_caption <- tm_map(docs_caption, stemDocument)
#Build a term-document matrix
dtm <- TermDocumentMatrix(docs_caption)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
d$dtm = dtm
return(d)
}
head(d, 10)
mention_freq = textMining(na.omit(nba_by_postcode$Mentions))
mention_freq = textMining(nba_by_postcode$Mentions)
mention_freq = textMining(na.omit(nba_by_postcode$Mentions))
head(mention_freq,10)[,c(1,2)]
summary(mention_freq$freq)
#Generate the Word cloud!!!
set.seed(1234)
wordcloud(words = mention_freq$word, freq = mention_freq$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35, scale =c(5,0,5) ,
colors=brewer.pal(8, "Dark2"))
wordcloud(words = mention_freq$word, freq = mention_freq$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35, scale =c(4,0,4) ,
colors=brewer.pal(8, "Dark2"))
wordcloud(words = mention_freq$word, freq = mention_freq$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35, scale =c(3,0,3) ,
colors=brewer.pal(8, "Dark2"))
